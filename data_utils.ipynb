{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import data\n",
    "We use pandas to read the .csv-files and store the data into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Import_Data(data_file, label_file):\n",
    "    data = pd.read_csv(data_file,header=None)\n",
    "    labels = pd.read_csv(label_file,header=None)\n",
    "    debug_msg = str('Imported data ('+str(data.shape)+') and labels ('+str(labels.shape)+').')\n",
    "    print(debug_msg)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature selection\n",
    "The training data contains a lot of bad features. For example, some columns have constant values and others have high correlation. To get rid of the worst features, we can use the feature_selection library in Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Zero_Variance(data):\n",
    "    selector = VarianceThreshold()\n",
    "    clean_data = selector.fit_transform(data)\n",
    "    debug_msg = str('Zero variance features removed from data. Input shape: ('+ str(data.shape)+'). Output shape: ('+str(clean_data.shape)+')')\n",
    "    print(debug_msg)\n",
    "    return clean_data\n",
    "\n",
    "def Select_Features_From_Model(data, labels, model_name='LassoCV'):\n",
    "    if model_name == 'LassoCV':\n",
    "        model = SelectFromModel(LassoCV())\n",
    "        model.fit(data, labels)\n",
    "        selected_features = model.transform(data)\n",
    "        debug_msg = str('Selected best features from input. Input shape: '+str(data.shape)+'. Output shape: '+str(selected_features.shape)+'.')\n",
    "        print(debug_msg)\n",
    "        return selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalization\n",
    "It is always good to normalize the data into some specific range. However, we need to be mindful of the methods of normalization and their effect on the distribution of data points in the given range. Below we have defined a function that utilizes multiple techniques for normalization.\n",
    "\n",
    "Min-max normalization and Z-score normalization are very common ways of normalizing and standardizing data. They are however sensitive to outliers. Therefore we also included tanh-normalization, which is discussed in (Latha, Thangasamy, 2011) (https://research.ijcaonline.org/volume32/number10/pxc3875530.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(data, type_string):\n",
    "    if type_string == 'min-max':\n",
    "        z = (data - data.mean()) / (data.max() - data.min())\n",
    "    elif type_string == 'z-score':\n",
    "        z = (data-data.mean())/data.std\n",
    "    elif type_string == 'tanh':\n",
    "        z = 0.5*(np.tanh(0.01*(data-data.mean)/data.std)+1)\n",
    "    else:\n",
    "        z = (data - data.mean()) / (data.max() - data.min())\n",
    "    debug_msg = str('Data normalized using ' + type_string + ' method.')\n",
    "    print(debug_msg)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Splitting into train and test sets\n",
    "We want to split the given training data into train and test sets, to be able to test performance locally, before submitting to Kaggle. For this we can use the model_selection library in Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_Data(data, labels, ratio=0.3, state=213):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=ratio, random_state=state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
